---
permalink: /
title: ""
author_profile: true
---

About
======

Hello :\) I am Erica, currently a Ph.D. candidate in Operations Research within the Department of Management Science & Engineering (MS&E) at [Stanford University](https://www.stanford.edu/). I am fortunate to be co-advised by Prof. [Jose Blanchet](https://web.stanford.edu/~jblanche/) (MS&E) and Prof. [Mert Pilanci](https://stanford.edu/~pilanci/) (Electrical Engineering). I am also grateful to be supported by the [Stanford Graduate Fellowship (SGF) in Science and Engineering](https://vpge.stanford.edu/fellowships-funding/sgf), generously sponsored by the [Koret Foundation](https://koret.org/grantees/cross-disciplinary-research-stanford-university/).

As part of my Ph.D., I recently completed a co-enrolled M.S. degree in Electrical Engineering (2025), specializing in Control & Optimization. Prior to Stanford, I earned a dual B.A. in Mathematics and Statistics from [Columbia College, Columbia University](https://www.college.columbia.edu/), graduating *summa cum laude* with honors from both departments.

*This summer, I’m excited to be interning as an Applied Scientist working on Generative AI with [Amazon Science](https://www.amazon.science/) at the Bellevue office.*

Research
======
My research lies at the intersection of optimization, statistics, and machine learning. As a theorist and statistician, I am particularly interested in revisiting and repurposing classical techniques from probability and optimization theory to tackle modern learning challenges, with a focus on improving sample efficiency in large-scale, high-dimensional settings.

Recently, my work has focused on the principled integration of large language models (LLMs) into traditional learning pipelines, leveraging their capacity to augment statistics-driven models with rich contextual metadata, while preserving tunability, robustness, and statistical rigor.

Philosophically, I’m inspired by mathematician [Hans Hahn's](https://en.wikipedia.org/wiki/Hans_Hahn_(mathematician)) view of mathematics as a precise, elegantly constructed conceptual framework: one that enables us to abstract information and perform tautological transformations to uncover fundamental laws governing our world. As I continue journey as a researcher, I hope to uncover more of these hidden structures within learning systems through the lenses of optimization and statistical theory and push the frontiers of what we can rigorously understand and design in machine learning.

Feel free to reach out if you're interested in my work 🙂

Scholarly Works
======
* <em>Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes</em><br>
  <small>
    <strong><u>Erica Zhang</u></strong><sup>*</sup>, Fangzhao Zhang<sup>*</sup>, Mert Pilanci<br>
    <em>International Conference on Machine Learning (ICML), 2025</em><br>
    [[PDF]](https://arxiv.org/pdf/2410.02145?) · [[arXiv]](https://arxiv.org/abs/2410.02145) · [[Codes]](https://github.com/pilancilab/cpal)
  </small>
* <em>LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization</em><br>
  <small>
    <strong><u>Erica Zhang</u></strong><sup>*</sup>, Naomi Sagan<sup>*</sup>, Ryan Goto<sup>*</sup>, Jurik Mutter, Nick Phillips, Ash Alizadeh, Kangwook Lee, Jose Blanchet, Mert Pilanci, Robert Tibshirani<br>
    <em>arXiv Preprint</em><br>
    [[PDF]](https://arxiv.org/pdf/2502.10648) · [[arXiv]](https://arxiv.org/abs/2502.10648) · [[Codes]](https://github.com/pilancilab/llm-lasso)
  </small>
* <em>HieroLM: Egyptian Hieroglyph Recovery with Next Word Prediction Language Model</em><br>
  <small>
    Xuheng Cai, <strong><u>Erica Zhang</u></strong><br>
    <em>LaTeCH-CLfL 2025 @ NAACL 2025</em><br>
    [[PDF]](https://arxiv.org/pdf/2503.04996) · [[arXiv]](https://arxiv.org/abs/2503.04996) · [[Codes]](https://github.com/Rick-Cai/HieroLM)
* <em>Empirical martingale projections via the adapted Wasserstein distance</em><br>
  <small>
    Jose Blanchet, Johannes Wiesel, <strong><u>Erica Zhang</u></strong>, Zhenyuan Zhang<br>
    <em>arXiv Preprint</em><br>
    [[PDF]](https://arxiv.org/pdf/2401.12197) · [[arXiv]](https://arxiv.org/abs/2401.12197) · [[Codes]](https://github.com/Ericavanee/Bicausal_Wasserstein_MtglProj)
  </small>
* <em>An optimal transport-based characterization of convex order</em><br>
  <small>
     Johannes Wiesel, <strong><u>Erica Zhang</u></strong><br>
    <em>Dependence Modeling 11 (1)</em><br>
    [[PDF]](https://arxiv.org/pdf/2207.01235) · [[arXiv]](https://arxiv.org/abs/2207.01235) · [[Codes]](https://github.com/johanneswiesel/Convex-Order)
  </small>
* <em>Convex Order and Arbitrage</em><br>
    <small>
     <strong><u>Erica Zhang</u></strong><br>
    <em>arXiv Preprint</em><br>
    [[PDF]](https://github.com/Ericavanee/Convex-Order-and-Arbitrage/blob/main/Convex_Order_and_Arbitrage%2C%20v1.pdf) · [[arXiv]](https://example.com/placeholder)
    </small>


Honors
======
- [Stanford Graduate Fellowship (SGF) in Sciences and Engineering](https://vpge.stanford.edu/fellowships-funding/sgf), Stanford University (2023)
- [Phi Beta Kappa](https://www.college.columbia.edu/node/19775) at Columbia College Delta Chapters, Columbia University (2023)
- Departmental Honors in Mathematics, Columbia University (2023)
- Departmental Honors in Statistics, Columbia University (2023)
- [Nexus Fellowship](https://campus.deshaw.com/), The D. E. Shaw Group (2021)
